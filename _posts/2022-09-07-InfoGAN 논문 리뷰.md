---
layout: post
title: "InfoGAN: Interpretable Representation Learning by Information Maximizing GAN 논문 리뷰"
summary: "Interpretable GAN 논문 리뷰"
author: taehun
date: '2022-09-07 17:00:00 +0900'
category: Literature_review
toc: true
toc_sticky: true
toc_label: "My Table of Contents"
toc_icon: "cog"
thumbnail: null
keywords: GAN, Interpretable GAN, InfoGAN
permalink: /20
mathjax: true
use_math: true
---

Last update: 2022.09.07<br>

> `InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets 리뷰'
> > 2016 NIPS Conference<br>

#### Index
---

- [1. Abstract](#1-abstract)
- [2. Mutual Information for Inducing Latent Codes](#2-mutual-information-for-inducing-latent-codes)<br><br>

#### **1. Abstract**
  
<br>

- InfoGAN은 GAN 모델에 정보 이론을 확장한 것으로, **unsupervised 방식으로 disentangled representation**을 학습할 수 있음
  - Small subset of latent variables와 observation 간에 mutual information을 극대화시키는 네트워크
  - 효율적으로 최적화가 가능한 mutual information objective의 lower bound를 유도함
  - 특히, InfoGAN은 성공적으로 데이터의 특징들을 잘 disentangle 시킴
    - ex) digit shape으로부터 writing style을, lighting of 3D rendered image로부터 pose를, central digit으로부터 background digit을 잘 분리함
  - 결과적으로, 현존하는 supervised methods에 의해 학습된 representation과 필적하는 interpretable representaion을 InfoGAN에서는 unsupervised manner로 달성할 수 있음

- 요약하면, generator의 입력으로 들어가는 noise 중 일부와 output observation 간에 mutual information을 objective function에 추가하고 최대화함으로써 interpretable & meaningful representations을 학습하도록 만들 수 있음
<br>

#### **2. Mutual Information for Inducing Latent Codes**

- **기존 GAN 문제점**
  - 기존 GAN에 들어가는 input noise vector z에는 어떠한 restrictions도 부과하지 않고 있기에, highly entangled된 방식으로 generator에서 학습이 되고, z의 개별 차원은 데이터의 semantic feature와도 일치하지 않는다.
  - 반면에, 많은 도메인들은 자연적으로 semantically meaningful factors로 분해된다. GAN 모델의 이상적인 상황을 예로 들어보면,
    - MNIST dataset을 생성할 때, 모델이 스스로 0~9에 대한 discrete r.v을 할당하고, digit의 각도, thickness에 대한 continuous r.vs를 할당하는 경우가 있을 것
    - 본 논문에서는, discrete한 10개의 값(0~9) 중 하나와, 각도, thickness를 표현하기 위한 2개의 continuous 값에 의해 MNIST digit이 생성되도록 만드는 방식을 사용함<br><br>

- **Mutual information 기반 해결 방법**
  - 구조화되지 않은, 일반적으로 사용하는 random sampling된 단일 noise vector를 사용하는 대신, input noise vector를 두 부류로 나눔
    - 1) z: source of incompressible noise
    - 2) c: latent code로, 데이터 분포의 중요한, 구조화된 semantic features를 targeting 함
      - Structured latent variables는 ![Lf](https://latex.codecogs.com/svg.latex?\small&space;c_{1},c_{2},...,c_{L})로 정의
      - Latent codes c의 distribution은 factored distribution으로 가정: ![Lf](https://latex.codecogs.com/svg.latex?\small&space;P(c_{1},c_{2},...,c_{L})=\prod P(c_{i}))
        - Factored distribution은, 전체 latent codes c에 대한 density는 각 latent code ![Lf](https://latex.codecogs.com/svg.latex?\small&space;c_{1},c_{2},...,c_{L})의 확률을 곱한 것으로 정의하는 것<br>
  - 본 논문에서는 latent factors를 발견하는 것을 unsupervised way로 해결하고자 함. 
    - 예를 들면, label에 대한 정보를 미리 설정하는 것이 아니라, 전체 class 개수가 10개라면 0~9에 대한 임의의 latent code를 활용해서 해당 label code에 따라 class가 바뀌도록 기대하는 것
    - 따라서, generator의 form도 ![Lf](https://latex.codecogs.com/svg.latex?\small&space;G(z,c))로 변함
  - 그러나, 일반적인 GAN의 경우 ![Lf](https://latex.codecogs.com/svg.latex?\small&space;P_{G}(x|c)=P_{G}(x))를 만족시키는 즉, additional latent code c가 아무 역할을 하지 못하고 noise vector z에 의해서 학습이 완료될 수 있음
  - 따라서, problem of trivial codes를 해결하기 위해 information-theoretic regularization을 제안
    - 이는, latent codes ![Lf](https://latex.codecogs.com/svg.latex?\small&space;c)와 generator distribution ![Lf](https://latex.codecogs.com/svg.latex?\small&space;G(z,c)) 사이에 mutual information이 커야한다는 것

<br>

- 
<br>

<p align="center">
  <img src="https://user-images.githubusercontent.com/86653075/179837645-66b3ebc3-a259-4fca-93ae-4f4064c942eb.png" width="400" height="auto">
</p>
